{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This example demonstrates how to run TensorFlow's Boosted Trees Estimator. Due to the nature of the\n",
    "model, this example is meant to run as a single-GPU model or a hyperparameter search; it does NOT\n",
    "support distributed training.\n",
    "Example based on this tutorial:\n",
    "    https://www.tensorflow.org/tutorials/estimator/boosted_trees\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "\n",
    "from determined.estimator import EstimatorTrial, EstimatorTrialContext\n",
    "\n",
    "\n",
    "class BoostedTreesTrial(EstimatorTrial):\n",
    "    def __init__(self, context: EstimatorTrialContext) -> None:\n",
    "        self.context = context\n",
    "\n",
    "        # Load Dataset.\n",
    "        (\n",
    "            self.dftrain,\n",
    "            self.dfeval,\n",
    "            self.y_train,\n",
    "            self.y_eval,\n",
    "            self.feature_columns,\n",
    "        ) = self.load_dataset()\n",
    "\n",
    "        # Wrap Optimizer (required by Determined but not used by this specific model).\n",
    "        self.context.wrap_optimizer(None)\n",
    "\n",
    "        # Set Hyperparameters - this is being populated at runtime from the .yaml configuration file.\n",
    "        self.n_trees = context.get_hparam(\"n_trees\")\n",
    "        self.max_depth = context.get_hparam(\"max_depth\")\n",
    "        self.learning_rate = context.get_hparam(\"learning_rate\")\n",
    "        self.l1_regularization = context.get_hparam(\"l1_regularization\")\n",
    "        self.l2_regularization = context.get_hparam(\"l2_regularization\")\n",
    "        self.min_node_weight = context.get_hparam(\"min_node_weight\")\n",
    "\n",
    "    def build_estimator(self) -> tf.estimator.Estimator:\n",
    "        # Since data fits into memory, use entire dataset per layer.\n",
    "        n_batches = 1\n",
    "\n",
    "        est = tf.estimator.BoostedTreesClassifier(\n",
    "            self.feature_columns,\n",
    "            n_batches_per_layer=n_batches,\n",
    "            n_trees=self.n_trees,\n",
    "            max_depth=self.max_depth,\n",
    "            learning_rate=self.learning_rate,\n",
    "            l1_regularization=self.l1_regularization,\n",
    "            l2_regularization=self.l2_regularization,\n",
    "            min_node_weight=self.min_node_weight,\n",
    "        )\n",
    "\n",
    "        return est\n",
    "\n",
    "    def make_input_fn(self, X, y, shuffle=True):\n",
    "        def input_fn():\n",
    "            NUM_EXAMPLES = len(y)\n",
    "\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n",
    "            dataset = self.context.wrap_dataset(dataset)\n",
    "\n",
    "            if shuffle:\n",
    "                dataset = dataset.shuffle(NUM_EXAMPLES)\n",
    "            dataset = dataset.repeat(1)\n",
    "            dataset = dataset.batch(NUM_EXAMPLES)\n",
    "\n",
    "            return dataset\n",
    "\n",
    "        return input_fn\n",
    "\n",
    "    def build_train_spec(self) -> tf.estimator.TrainSpec:\n",
    "        return tf.estimator.TrainSpec(\n",
    "            self.make_input_fn(self.dftrain, self.y_train, shuffle=True)\n",
    "        )\n",
    "\n",
    "    def build_validation_spec(self) -> tf.estimator.EvalSpec:\n",
    "        return tf.estimator.EvalSpec(\n",
    "            self.make_input_fn(self.dfeval, self.y_eval, shuffle=False),\n",
    "            steps=None\n",
    "        )\n",
    "\n",
    "\n",
    "    def load_dataset(self):\n",
    "    \n",
    "        dftrain = pd.read_csv(\n",
    "            self.context.get_data_config()[\"titanic_dataset\"][\"train\"]\n",
    "        )\n",
    "        dfeval = pd.read_csv(\n",
    "            self.context.get_data_config()[\"titanic_dataset\"][\"eval\"]\n",
    "        )\n",
    "        y_train = dftrain.pop(\"survived\")\n",
    "        y_eval = dfeval.pop(\"survived\")\n",
    "    \n",
    "        CATEGORICAL_COLUMNS = [\n",
    "            \"sex\",\n",
    "            \"n_siblings_spouses\",\n",
    "            \"parch\",\n",
    "            \"class\",\n",
    "            \"deck\",\n",
    "            \"embark_town\",\n",
    "            \"alone\",\n",
    "        ]\n",
    "        NUMERIC_COLUMNS = [\"age\", \"fare\"]\n",
    "    \n",
    "        def one_hot_cat_column(feature_name, vocab):\n",
    "            return tf.feature_column.indicator_column(\n",
    "                tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "                    feature_name, vocab\n",
    "                )\n",
    "            )\n",
    "    \n",
    "        feature_columns = []\n",
    "    \n",
    "        for feature_name in CATEGORICAL_COLUMNS:\n",
    "            # Need to one-hot encode categorical features.\n",
    "            vocabulary = dftrain[feature_name].unique()\n",
    "            feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\n",
    "    \n",
    "        for feature_name in NUMERIC_COLUMNS:\n",
    "            feature_columns.append(\n",
    "                tf.feature_column.numeric_column(feature_name, dtype=tf.float32)\n",
    "            )\n",
    "    \n",
    "        return dftrain, dfeval, y_train, y_eval, feature_columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
